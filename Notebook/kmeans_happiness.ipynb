{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3724ac30-0f43-4635-9c2c-943495b9b5f3",
   "metadata": {},
   "source": [
    "# Unsupervised Machine Learning \n",
    "\n",
    "**Unsupervised learning** is a branch of machine learning where we do not have any output/dependent/target variables, we only work with input/independent/feature variables. In the context of unsupervised learning, and particularly in clustering tasks, the goal is to group data points into clusters, smaller subsets of similar data, from which we can draw useful insights or patterns.\n",
    "\n",
    "## KMeans Clustering\n",
    "KMeans clustering is an example of **exclusive clustering**, meaning each observation can belong to exactly one cluster only. There is no overlap. An observation cannot be part of two clusters simultaneously.\n",
    "\n",
    "This algorithm works only with numerical variables, which means all non-numeric features should be removed before applying KMeans. Additionally, feature values should be either standardized or normalized, depending on the context. This step ensures that variables with larger scales do not dominate the clustering process, as KMeans relies on Euclidean distance, which is highly sensitive to variable scale.\n",
    "\n",
    "Even if the dataset does not contain outliers, scaling remains essential. Without it, variables with larger numerical ranges can disproportionately influence the clustering results, leading to biased groupings.\n",
    "\n",
    "However, if outliers are present, they should be treated before scaling. One commonly used technique is Winsorization, which reduces the impact of extreme values by capping them at specified percentiles. Once outliers are handled, it is safe to proceed with normalization or standardization.\n",
    "\n",
    "To determine the optimal number of clusters K, the elbow method is most frequently used. Alternatively, a direct metric-based approach can be applied, such as maximizing the ratio:\n",
    "betweenss / totss,\n",
    "which reflects how well-separated the clusters are. Higher values of this ratio indicate better-defined clustering structures.\n",
    "\n",
    "In **R**, the `kmeans()` function is commonly used for this purpose. It has several important parameters such as:\n",
    "- the number of clusters (`centers`) we want to use,\n",
    "- the number of times the algorithm will run with different initial cluster positions (`nstart`).\n",
    "- the maximum number of iterations the algorithm will run (`iter.max`)\n",
    "The number of clusters is usually determined by the **elbow method** or by using a simple metric. The most commonly used metric is the ratio: `betweenss / totss`.\n",
    "\n",
    "Unlike supervised learning, clustering does not have well-defined evaluation metrics like accuracy, precision, or recall. Instead, we focus on:\n",
    "- Within-cluster sum of squares (withinss): the sum of squared distances between each observation and the centroid of its cluster.\n",
    "- Between-cluster sum of squares (betweenss): the sum of squared distances between each cluster’s centroid and the overall centroid (global center).\n",
    "- Total sum of squares (totss): the total squared distance of all observations from the global centroid.\n",
    "\n",
    "The goal is to maximize the ratio `betweenss / totss`, ideally approaching 100%, but of course, that is an ideal scenario (utopia) that is rarely achievable in practice.\n",
    "\n",
    "The KMeans algorithm works by iteratively updating cluster centroids. We specify a maximum number of iterations, and the algorithm stops early if the centroids do not change significantly between iterations, meaning it has converged.\n",
    "\n",
    "Another important parameter is `nstart`, which determines how many random initial centroid positions will be tested. Since the initial positions are selected randomly, setting `nstart = 1` is discouraged, it is always better to try multiple initializations to avoid poor local minima.\n",
    "Parameter `iter.max` is used to set an upper bound on the number of iterations the algorithm will run. This is important because if the initial cluster centers are poorly chosen (due to random selection), the algorithm might struggle to converge, meaning it can’t find optimal cluster centers so this parameter acts as a safeguard to stop the algorithm after a certain number of iterations, even if convergence hasn’t been reached.\n",
    "\n",
    "### Implementation in Python\n",
    "\n",
    "Having discussed the KMeans algorithm in the R environment, we will now demonstrate its implementation in **Python** using the `scikit-learn` library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252e80b3-7606-48ae-96d8-1fb359a3bdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans clustering on world happiness data\n",
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Loading dataset\n",
    "data = pd.read_csv('Data/world-happiness-report-2021.csv')\n",
    "\n",
    "# Showing first 5 rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74121685-d2ca-4098-9667-6de918ea15e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using info to obtain more information about the dataset\n",
    "# We can see 2 categorical variables which are not suitable for KMeans clustering\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cbe9d1-e49f-4b01-8cfd-f18247edc242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing variables which are not useful \n",
    "data = data.drop(columns=[\n",
    "    \"Country name\",  # categorical variable\n",
    "    \"Regional indicator\", # categorical variable\n",
    "    \"Ladder score\", # aggregate metric calculated based on other columns\n",
    "    \"Standard error of ladder score\",  # margin of error for ladder score estimation\n",
    "    \"upperwhisker\",  # upper limit for ladder score\n",
    "    \"lowerwhisker\",  # lower limit for ladder score\n",
    "    \"Ladder score in Dystopia\", # no direct meaning for our model\n",
    "    # these 6 would represent duplicated information\n",
    "    \"Explained by: Log GDP per capita\",\n",
    "    \"Explained by: Social support\",\n",
    "    \"Explained by: Healthy life expectancy\",\n",
    "    \"Explained by: Freedom to make life choices\",\n",
    "    \"Explained by: Generosity\",\n",
    "    \"Explained by: Perceptions of corruption\",\n",
    "    \"Dystopia + residual\" # hard to interpret\n",
    "])\n",
    "\n",
    "# Checking for missing values \n",
    "missing_values = data.isna().sum()\n",
    "print(\"Missing values per column:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a82c5e1-5d49-4b34-b636-04596f32b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting histograms for all features\n",
    "data.hist(bins=30, figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Based on histogram plots most features are not normally distributed\n",
    "# Therefore, IQR is a more appropriate method for outlier detection than z-score.\n",
    "\n",
    "def calculate_iqr_bounds(series):\n",
    "    # Using numpy.quantile \n",
    "    q1 = np.quantile(series, 0.25)\n",
    "    q3 = np.quantile(series, 0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "    return lower, upper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d84b39-6a02-4e39-a962-0ffb4795bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting outliers in all columns\n",
    "outliers_dict = {}\n",
    "\n",
    "for col in data.columns:\n",
    "    lower, upper = calculate_iqr_bounds(data[col])\n",
    "    outliers = data[(data[col] < lower) | (data[col] > upper)][col]\n",
    "    outliers_dict[col] = outliers\n",
    "\n",
    "# Number of outliers per column\n",
    "for col, outliers in outliers_dict.items():\n",
    "    print(f\"{col}: {len(outliers)} outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023215e4-f4c4-4038-bcf6-1bed6d01dd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying outliers for a single column\n",
    "# The column Perceptions of corruption only has lower outliers, it means that there are a few countries with unusually low\n",
    "# perceived corruption values.\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(x=data[col])\n",
    "plt.title(f\"Boxplot - {col}\")\n",
    "plt.xlabel(col)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dd3803-dfee-4936-b4a6-858a5bbbee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Function to cap outliers using IQR method by capping extreme values\n",
    "def clip_outliers_iqr(dataframe):\n",
    "    for col in dataframe.columns:\n",
    "        # Check if the column is numeric to avoid errors\n",
    "        if pd.api.types.is_numeric_dtype(dataframe[col]):\n",
    "            lower, upper = calculate_iqr_bounds(dataframe[col].to_numpy(dtype='float64', copy=True))\n",
    "            # Cap outliers (Winsorization)\n",
    "            dataframe[col] = np.clip(dataframe[col], lower, upper)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6033862f-8541-441c-aded-4db032ba7f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appling Winsorization on dataset\n",
    "data = clip_outliers_iqr(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2057462d-cfbc-4b40-a53e-b88ab673b6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting to visually confirm that outliers are capped\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(x=data[col])\n",
    "plt.title(f\"Boxplot - {col}\")\n",
    "plt.xlabel(col)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d76238-fccf-47e5-b9ff-140f9bd2db28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving on to feature scaling using standardization\n",
    "\n",
    "# Both standardization (StandardScaler) and normalization (MinMaxScaler) were tested for feature scaling before clustering.\n",
    "# While normalization showed a slightly better silhouette score for K = 3,\n",
    "# standardization produced more consistent and stable results across the full range of cluster values.\n",
    "# Differences were minor so I decided to go with standardization\n",
    "scaler = StandardScaler()\n",
    "data[data.columns] = scaler.fit_transform(data)\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb269c3-caed-484c-b082-9d8e82dbf627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the range of clusters to test\n",
    "# Inertia is equivalent to tot.withinss in R\n",
    "inertia = [] \n",
    "k_range = range(2, 9)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=1000)\n",
    "    kmeans.fit(data)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plotting the Elbow graph to determine the optimal number of clusters\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_range, inertia, marker='o')\n",
    "plt.title('Elbow Method for Optimal k (k = 2 to 8)')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia (WSS)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63a8e54-c6f3-4538-ad2c-24507419eac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "# In this section, I evaluate different values of K using the silhouette score.\n",
    "# It measures how well each data point fits within its assigned cluster\n",
    "# compared to other clusters. The score ranges from -1 to 1.\n",
    "# Score close to 1 indicates that the point is well clustered.\n",
    "# Score close to 0 means it's on the boundary between two clusters.\n",
    "# Negative score means it may have been assigned to the wrong cluster.\n",
    "\n",
    "\n",
    "silhouette_scores = []\n",
    "\n",
    "# Test values of k from 2 to 8\n",
    "for k in range(2, 9):\n",
    "    kmeans = KMeans(n_clusters=k, max_iter=20, n_init=1000, random_state=4)\n",
    "    cluster_labels = kmeans.fit_predict(data)\n",
    "    score = silhouette_score(data, cluster_labels)\n",
    "    silhouette_scores.append((k, score))\n",
    "\n",
    "\n",
    "silhouette_df = pd.DataFrame(silhouette_scores, columns=['k', 'silhouette_score'])\n",
    "\n",
    "\n",
    "print(silhouette_df)\n",
    "\n",
    "# Plotting silhouette scores\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(silhouette_df['k'], silhouette_df['silhouette_score'], marker='o')\n",
    "plt.title('Silhouette Score vs. Number of Clusters')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160e19dd-c44a-45e3-80a1-e15674628aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting clustering when K = 2\n",
    "kmeans_2 = KMeans(n_clusters=2, n_init=100, max_iter=20, random_state=42)\n",
    "clusters_2 = kmeans_2.fit_predict(data)\n",
    "\n",
    "\n",
    "#  Plotting clustering when K = 3\n",
    "kmeans_3 = KMeans(n_clusters=3, n_init=100, max_iter=20, random_state=42)\n",
    "clusters_3 = kmeans_3.fit_predict(data)\n",
    "\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(data)\n",
    "\n",
    "\n",
    "# Plotting side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot for K = 2\n",
    "axes[0].scatter(pca_result[:, 0], pca_result[:, 1], c=clusters_2, cmap='Set1', s=60)\n",
    "axes[0].set_title('Clusters K=2')\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot for K = 3\n",
    "axes[1].scatter(pca_result[:, 0], pca_result[:, 1], c=clusters_3, cmap='Set1', s=60)\n",
    "axes[1].set_title('Clusters K=3')\n",
    "axes[1].set_xlabel('PC1')\n",
    "axes[1].set_ylabel('PC2')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Based on the PCA plots, K = 2 appears to provide slightly more distinct cluster separation compared to K = 3.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddd6b18-b894-4413-811a-9b146ffc016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering using optimal number of clusters\n",
    "kmeans_final = KMeans(n_clusters=2, n_init=100, max_iter=20, random_state=42)\n",
    "data['Cluster'] = kmeans_final.fit_predict(data)\n",
    "# fits the KMeans model and returns the cluster label 0 or 1 for each row\n",
    "\n",
    "cluster_summary = data.groupby('Cluster').mean()\n",
    "print(cluster_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec7ff4b-b646-4573-8d95-c7008382535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.heatmap(cluster_summary, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Cluster Profile Heatmap')\n",
    "plt.ylabel('Cluster')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325029b9-21cc-4b2b-8765-d1c7bb7f415e",
   "metadata": {},
   "source": [
    "### Cluster interpretation\n",
    "\n",
    "To better understand the nature of the clusters formed via KMeans, I calculated the average z-scored values for the key happiness related features and visualized them using a heatmap.\n",
    "Z-score measures the deviation from the mean and it's expressed in the units of standard deviation.\n",
    "Z-score allows us to compare variables on the same scale even if their original units were different.\n",
    "\n",
    "Cluster 0 shows positive z-scores for: \n",
    "    Logged GDP per capita (+0.57)\n",
    "    Social support (+0.57)\n",
    "    Healthy life expectancy (+0.59)\n",
    "    Freedom to make life choices (+0.35)\n",
    "\n",
    "These values indicate that countries in Cluster 0 tend to have strong economic indicators, better health, more freedom.\n",
    "\n",
    "Cluster 1 shows negative z-scores for:\n",
    "    GDP (-1.06)\n",
    "    Social support (-1.06)\n",
    "    Life expectancy (-1.10)\n",
    "    Freedom (-0.65)\n",
    "\n",
    "This suggests that Cluster 1 includes countries with weaker economic and health related metrics while they display greater generosity and higher trust in institutions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b14e3a2-ff81-4f31-bfaf-17e793f11a17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
